{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "\n",
    "# For XRF, ignore repeats and standards\n",
    "xrf = pd.read_excel('raw/GAL-DV-21_XRF.xlsx',usecols='A:BB',index_col=0)\n",
    "\n",
    "# For ICPMS, ignnore repeats and standards\n",
    "icpms = pd.read_excel('raw/DV-21-ICPMS.xlsx',index_col=0,nrows=54)\n",
    "\n",
    "# Make directory for processed data\n",
    "os.makedirs('processed',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull unnormalized major elements from XRF and transpose\n",
    "majors_unnorm = xrf.iloc[5:15,:].T\n",
    "\n",
    "# Pull normalized major elements from XRF and transpose to make samples index\n",
    "majors_norm = xrf.iloc[19:29,:].T\n",
    "\n",
    "# Pull LOI from XRF and transpose \n",
    "loi = xrf.iloc[16,:].T\n",
    "\n",
    "# Remove extra space in majors_unnorm columns\n",
    "new_cols = majors_unnorm.columns.str.replace(' ','')\n",
    "new_cols_unnorm = [x+'_unnorm' for x in new_cols]\n",
    "majors_unnorm_corr = majors_unnorm.copy()\n",
    "majors_unnorm_corr.columns = new_cols_unnorm\n",
    "\n",
    "# Remove extra space in majors_norm columns\n",
    "new_cols = majors_norm.columns.str.replace(' ','')\n",
    "majors_norm_corr = majors_norm.copy()\n",
    "majors_norm_corr.columns = new_cols\n",
    "\n",
    "# Pull XRF trace elements and transpose\n",
    "xrf_trace = xrf.iloc[32:51,:].T\n",
    "\n",
    "# Remove extra space in XRF Trace columns\n",
    "new_cols = xrf_trace.columns.str[1:]\n",
    "xrf_trace_corr = xrf_trace.copy()\n",
    "xrf_trace_corr.columns = new_cols\n",
    "\n",
    "# Remove 'ppm' from ICPMS columns\n",
    "new_cols = icpms.columns.str[:-4]\n",
    "icpms_corr = icpms.copy()\n",
    "icpms_corr.columns  = new_cols\n",
    "\n",
    "# Remove XRF data duplicated by ICPMS\n",
    "common_cols = xrf_trace_corr.columns.intersection(icpms_corr.columns)\n",
    "xrf_trace_culled = xrf_trace_corr.drop(common_cols,axis=1)\n",
    "\n",
    "# Check what is in each file\n",
    "print(majors_unnorm_corr.columns)\n",
    "print(majors_norm_corr.columns)\n",
    "print(loi.name)\n",
    "print(xrf_trace_culled.columns)\n",
    "print(icpms_corr.columns)\n",
    "print(majors_norm_corr.index.equals(xrf_trace_culled.index))\n",
    "print(xrf_trace_culled.index.equals(icpms_corr.index))\n",
    "\n",
    "# Combine into single dataframe\n",
    "data_organized = pd.concat([majors_unnorm_corr,majors_norm_corr,loi,xrf_trace_culled,icpms_corr],axis=1)\n",
    "print(data_organized.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in metadata\n",
    "meta = pd.read_csv('metadata/gchm_smps_long.csv',index_col=0)\n",
    "\n",
    "# Isolate columns of interest\n",
    "meta_cols = ['Latitude','Longitude','Elevation','Rock_Type','Period','S_Domain']\n",
    "\n",
    "# Create placeholder rows for 184 and 186 for now\n",
    "meta.loc['G22184',:] = 0\n",
    "meta.loc['G22186',:] = 0\n",
    "\n",
    "# Fix the misspelled Khaishi\n",
    "typo = meta[meta['S_Domain']=='Khashi'].index\n",
    "print(typo)\n",
    "meta.loc[typo,'S_Domain'] = 'Khaishi'\n",
    "\n",
    "# Isolate rows that were actually run\n",
    "meta_trimmed = meta.loc[data_organized.index,meta_cols]\n",
    "\n",
    "# Add metadata to main table\n",
    "data_final = data_organized.join(meta_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with 2019 data\n",
    "data_2019 = pd.read_csv('processed/data_2019.csv',index_col=0)\n",
    "\n",
    "data_combined = pd.concat([data_final,data_2019])\n",
    "print(data_combined.index)\n",
    "print(data_combined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in lithologies from thin sections\n",
    "lith = pd.read_csv('metadata/lithology.csv',index_col=0)\n",
    "data_combined.loc[lith.index,'Rock_Type'] = lith['Rock Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "data_combined.to_csv('processed/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from Gale13\n",
    "data = pd.read_excel('published/galeetal_supplementarytables_final.xlsx',sheet_name='5- Master TE-Iso Compilation.')\n",
    "\n",
    "# isolate back-arc basins\n",
    "bab_data = data[data['Seg Name'].astype(str).str.startswith('B')]\n",
    "\n",
    "# Assign nan to empty cells\n",
    "bab_data.replace(' ',np.nan,inplace=True)\n",
    "\n",
    "# Save modified data\n",
    "bab_data.to_csv('processed/data_gale.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-caucasus-gchem-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
